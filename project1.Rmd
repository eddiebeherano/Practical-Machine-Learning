---
title: "project1"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}

library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)

#Setting working directory
setwd("C:/Users/adria/Desktop/R data folder/Practical Machine Learning")

#Setting random seed
set.seed(43244)

#Importing the data from the URL

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))

#Partioning the Training data set into two data sets, 60% for myTrain, 40% for myTest:
  
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTrain <- training[inTrain, ]
myTest <- training[-inTrain, ]

#Checking the dimensions
dim(myTrain)
dim(myTest)

#Cleaning the data

# Cleaning NearZeroVariance Variables

#Finding possible non-zero variance Variables:
nzvdata <- nearZeroVar(myTrain, saveMetrics=TRUE)

#Run this code to create another subset without NZV variables:

myNZV <- names(myTrain) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
                                      "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
                                      "max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
                                      "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
                                      "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
                                      "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
                                      "max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
                                      "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
                                      "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
                                      "amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
                                      "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
                                      "max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
                                      "amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
                                      "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
                                      "stddev_yaw_forearm", "var_yaw_forearm")
myTrain <- myTrain[!myNZV]

#Checking the number of observations
dim(myTrain)

#Removing first column of the data so that it does not interfere with the algorithm
  
myTrain <- myTrain[c(-1)]

#Cleaning Variables which have NAs more than 70% of data. 

trainingV3 <- myTrain #creating another subset to iterate in loop
for(i in 1:length(myTrain)) { #for every column in the training dataset
  if( sum( is.na( myTrain[, i] ) ) /nrow(myTrain) >= .7 ) { #if n?? NAs > 60% of total observations
    for(j in 1:length(trainingV3)) {
      if( length( grep(names(myTrain[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
        trainingV3 <- trainingV3[ , -j] #Remove that column
      }   
    } 
  }
}

#Checking the number of observations in the new dataset:
dim(trainingV3)

#making the new training dataset
myTrain <- trainingV3
rm(trainingV3)

#Redoing the last step but fort the testing data sets:

step1 <- colnames(myTrain)
step2 <- colnames(myTrain[, -58]) #already with classe column removed
myTest <- myTest[step1]
testing <- testing[step2]

#Checking the dimensions of the cleaned data :
dim(myTest)


#Coercing the data into the same class type in order to ensure that ML algorithms will work :

for (i in 1:length(testing) ) {
  for(j in 1:length(myTrain)) {
    if( length( grep(names(myTrain[i]), names(testing)[j]) ) ==1)  {
      class(testing[j]) <- class(myTrain[i])
    }      
  }      
}

#Making sure coertion worked :
testing <- rbind(myTrain[2, -58] , testing) 
testing <- testing[-1,]

# 3 ) Using Decision Tree algorithm to predict : 

modFitA1 <- rpart(classe ~ ., data=myTrain, method="class")

#Viewing the decision tree
  
fancyRpartPlot(modFitA1)

  
#Predicting:
  
predictionsA1 <- predict(modFitA1, myTest, type = "class")

# Using confusion matrix to test results:
    
confusionMatrix(predictionsA1, myTest$classe)

# 4) Using Random Forests for prediction
modFitB1 <- randomForest(classe ~. , data=myTrain)

#Predicting in sample error:
  
predB1 <- predict(modFitB1, myTest, type = "class")

#Using confusion Matrix to test results:
  
confusionMatrix(predB1, myTest$classe)

#We conclude that the Random Forests got a better result

#5) Finally, using the provided Test Set out-of-sample error.

#Using the following formula which made a much better prediction in in-sample:

predB2 <- predict(modFitB1, testing, type = "class")

#Finally : Generating the predictions for the assignment

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predB2)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
